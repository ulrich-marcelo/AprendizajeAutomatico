{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizaje Automático\n",
    "#### Segundo cuatrimestre 2025\n",
    "\n",
    "## Regresión Lineal\n",
    "\n",
    "#### Bibliografía: \n",
    "* Pattern Recognition and Machine Learning, Christopher M. Bishop. Capíulo 3.\n",
    "* The Elements of Statistical Learning, Trevor Hastie, Robert Tibshirani, Jerome Friedman. Capítulo 3.\n",
    "\n",
    "Papers:\n",
    "* Ridge Regression: Biased Estimation for Nonorthogonal Problems, Arthur E. Hoerl and Robert W. Kennard, 1970.\n",
    "* Regression Shrinkage and Selection via the Lasso, Robert Tibshirani, 1996.\n",
    "* Regularization and variable selection via the elastic net, Hui Zou and Trevor Hastie, 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regresión Lineal: formulación\n",
    "\n",
    "La regresión lineal es un modelo que asume que la relación entre las variables de entrada $X^T = (X_1, X_2,..., X_p)$ y la variable de salida $Y$ es lineal. Entonces, toma la forma:\n",
    "\n",
    "$$ f(X) = \\beta_0 + \\sum_{j=1}^{p} X_j \\beta_j$$\n",
    "\n",
    "En general, se suele escribir de forma más compacta como\n",
    "\n",
    "$$ f(X) = X^T \\beta$$\n",
    "\n",
    "donde $X$ es un vector de tamaño $p+1$ que contiene un 1 en la primera posición y los valores de las variables de entrada en las siguientes posiciones. $\\beta = (\\beta_0, \\beta_1, ..., \\beta_p)$ es el vector de coeficientes a estimar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**¿Qué estamos asumiendo con esta $f(X)$?**\n",
    "\n",
    "Estamos asumiendo que la función de regresión $E(Y|X)$ es lineal o que **el modelo lineal es una buena aproximación**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**¿Qué son los $X$ ?**\n",
    "\n",
    "Prácticamente cualquier cosa que se nos ocurra:\n",
    "* Variables continuas.\n",
    "* Expansiones polinómicas de variables continuas u otros tipos de transformaciones.\n",
    "* Variables categóricas apropiadamente codificadas.\n",
    "* Interacciones entre variables.\n",
    "\n",
    "Lo importante es que el modelo es lineal en los parámetros, no necesariamente en las variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Dado un conjunto de entrenamiento $\\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}$, el objetivo es encontrar los coeficientes $\\beta$ que mejor ajusten a esos datos. Tener en cuenta que cada $x_i = (x_{i1}, x_{i2}, ..., x_{ip})$ es el vector de atributos para la observación $i$ (a veces con un 1 en la primera posición).\n",
    "\n",
    "*mejor ajusten a los datos* ---> En general significa minimizar una función de costo. En este caso, la función de costo más común es la suma de los residuos al cuadrado (RSS):\n",
    "\n",
    "$$RSS = \\sum_{i=1}^{N} (y_i - f(x_i))^2$$\n",
    "\n",
    "y el método más común para encontrar los coeficientes es el de **cuadrados mínimos**, que consiste en minimizar RSS respecto de $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entrenamiento - Cuadrados mínimos\n",
    "\n",
    "Utilizando notación matricial, podemos reescribir la suma de los residuos al cuadrado como:\n",
    "\n",
    "$$ RSS (\\beta) = ||\\textbf{y} - \\textbf{X} \\beta||^2 = (\\textbf{y} - \\textbf{X} \\beta)^T (\\textbf{y} - \\textbf{X} \\beta) $$\n",
    "\n",
    "donde es explícita la dependencia de la función de costo respecto de los parámetros $\\beta$.\n",
    "\n",
    "Si diferenciamos respecto de $\\beta$, obtenemos la primera derivada:\n",
    "\n",
    "$$\\frac{\\partial RSS}{\\partial \\beta} = -2 \\textbf{X}^T (\\textbf{y} - \\textbf{X} \\beta)$$\n",
    "\n",
    "y, diferenciando nuevamente, la segunda derivada:\n",
    "\n",
    "$$\\frac{\\partial^2 RSS}{\\partial \\beta^2}^* = 2 \\textbf{X}^T \\textbf{X}$$\n",
    "\n",
    "*(dejamos de lado un $\\beta^T$ para simplificar la notación)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entrenamiento - Cuadrados mínimos\n",
    "\n",
    "Si, además, $\\textbf{X}$ tiene rango completo, entonces $\\textbf{X}^T \\textbf{X}$ es definida positiva, lo que implica que la función de costo es convexa y tiene un mínimo que podemos obtener igualando la segunda derivada a cero (ecuación normal):\n",
    "\n",
    "$$\\textbf{X}^T (\\textbf{y} - \\textbf{X} \\beta) = 0  $$\n",
    "\n",
    "cuya solución es:\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "De esta forma, las predicciones $\\hat{\\textbf{y}}$ se obtienen como:\n",
    "\n",
    "$$\\hat{\\textbf{y}} = \\textbf{X} \\hat{\\beta} = \\textbf{X} (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "Desde un punto de vista operativo, ya está. Obtuvimos un estimador de los coeficientes $\\hat{\\beta}$ que nos sirve para hacer predicciones. No hay, prácticamente, ninguna hipótesis sobre los datos, más allá de que $\\textbf{X}$ tiene rango completo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Derivando matrices\n",
    "\n",
    "$$ (\\textbf{y} - \\textbf{X} \\beta)^T (\\textbf{y} - \\textbf{X} \\beta) = (\\textbf{y}^T - \\beta^T \\textbf{X}^T)(\\textbf{y} - \\textbf{X} \\beta)$$\n",
    "\n",
    "$$ (\\textbf{y}^T - \\beta^T \\textbf{X}^T)(\\textbf{y} - \\textbf{X} \\beta) = \\textbf{y}^T \\textbf{y} - \\textbf{y}^T \\textbf{X} \\beta - \\beta^T \\textbf{X}^T \\textbf{y} + \\beta^T \\textbf{X}^T \\textbf{X} \\beta$$\n",
    "\n",
    "Considerar que tanto $\\textbf{y}^T \\textbf{X} \\beta$ como $\\beta^T \\textbf{X}^T \\textbf{y}$ son escalares, por lo que se cumple que $\\textbf{y}^T \\textbf{X} \\beta = (\\textbf{y}^T \\textbf{X} \\beta)^T = \\beta^T \\textbf{X}^T \\textbf{y}$. Entonces, \n",
    "\n",
    "$$ (\\textbf{y} - \\textbf{X} \\beta)^T (\\textbf{y} - \\textbf{X} \\beta) = \\textbf{y}^T \\textbf{y} - 2 \\textbf{y}^T \\textbf{X} \\beta + \\beta^T \\textbf{X}^T \\textbf{X} \\beta.$$\n",
    "\n",
    "\n",
    "Como la derivada tiene tantos componentes como $\\beta$, que es **vector columna**, debemos obtener un vector columna. Tener en cuenta la siguiente propiedad $\\frac{\\partial}{\\partial \\beta} (a^T \\beta) = a$ si $a$ no depende de $\\beta$.\n",
    "\n",
    "Entonces, si diferenciamos respecto de $\\beta$, obtenemos:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} (\\textbf{y}^T \\textbf{y}) = 0$$\n",
    "$$ \\frac{\\partial}{\\partial \\beta} (-2 \\textbf{y}^T \\textbf{X} \\beta) = -2 \\textbf{X}^T \\textbf{y}$$\n",
    "$$ \\frac{\\partial}{\\partial \\beta} (\\beta^T \\textbf{X}^T \\textbf{X} \\beta) = 2 \\textbf{X}^T \\textbf{X} \\beta$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometría de los resultados\n",
    "<img src=\"Figuras/Geometría cuadrados mínimos.png\" alt=\"Figura 1\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometría de los resultados\n",
    "\n",
    "* Los vectores columnas de $\\textbf{X}$ generan un subespacio en $\\mathbb{R}^N$. *Pregunta: ¿cuál es su dimensión?*\n",
    "* De $\\textbf{X}^T (\\textbf{y} - \\textbf{X} \\beta) = 0 $ se deduce que el vector $\\textbf{y} - \\hat{\\textbf{y}}$ es ortogonal a este subespacio.\n",
    "    * Por lo tanto, la solución $\\hat{\\beta}$ es el vector que minimiza la distancia entre $\\textbf{y}$ y el espacio generado por los vectores columnas de $\\textbf{X}$.\n",
    "\n",
    "Entonces, $\\hat{\\textbf{y}}$ es la proyección ortogonal de $\\textbf{y}$ en ese subespacio. La matriz encargada de hacer esa proyección es $\\textbf{H}$, conocida como **matriz de proyección** o **matriz hat**.\n",
    "\n",
    "$$ \\hat{\\textbf{y}} = \\textbf{X} \\hat{\\beta} = \\textbf{X} (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "¿Cuándo hay problemas? Si las columnas de $\\textbf{X}$ no son linealmente independientes -  no tiene rango completo - $\\textbf{X}^T \\textbf{X}$ no es invertible, por lo que puede haber más de un $\\hat{\\beta}$. Sin embargo, se sigue cumpliendo que $\\hat{\\textbf{y}}$ siguen siendo la proyección de $\\textbf{y}$ en ese subespacio, solamente que hay más de una forma de expresarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un poco de estadística\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "En general, dado un estimador, se suele estudiar su esperanza, varianza y distribución:\n",
    "\n",
    "* La esperanza sirve para ver si el estimador es sesgado o no. Un estimador $\\hat{\\beta}$ es insesgado si $E(\\hat{\\beta}) = \\beta$.\n",
    "* La varianza mide la dispersión del estimador alrededor de su valor esperado. Si la varianza es baja, el estimador es más confiable.\n",
    "* La distribución nos dice cómo se comporta el estimador en diferentes muestras. Sirve, por ejemplo, para construir intervalos de confianza.\n",
    "\n",
    "Para calcular cada uno de estos valores, es necesario hacer algunas suposiciones sobre el modelo (generalmente, de manera incremental). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un poco de estadística\n",
    "\n",
    "Para calcular cada uno de estos valores, es necesario hacer algunas suposiciones sobre el modelo (generalmente, de manera incremental). \n",
    "\n",
    "Asumimos que:\n",
    "\n",
    "1. El modelo lineal es correcto:\n",
    "     $$Y = E(Y|X_1, ..., X_p) + \\epsilon$$\n",
    "     $$Y = \\beta_0 + \\sum_{j=1}^{p} X_j \\beta_j + \\epsilon$$\n",
    "1. Las observaciones $y_i$ son independientes entre sí, con varianza constante $\\sigma^2$.\n",
    "1. Los $x_i$ son fijos y no aleatorios.\n",
    "1. Los errores siguen una distribución normal: $\\epsilon \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "* En algunas circunstancias, será conveniente escribir que $Var(\\epsilon) = \\sigma^2 I$, donde $I$ es la matriz identidad.\n",
    "* Se pueden relajar algunos supuestos para obtener resultados similares, pero no es el objetivo de esta materia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un poco de estadística\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T (\\textbf{X} \\beta + \\epsilon)$$\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{X} \\beta + (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon$$\n",
    "\n",
    "$$\\hat{\\beta} = \\beta + (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon$$\n",
    "\n",
    "**Esperanza**\n",
    "$$E(\\hat{\\beta}) = E(\\beta + (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon)$$\n",
    "\n",
    "$$E(\\hat{\\beta}) = \\beta + E((\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon)$$\n",
    "\n",
    "Como $E(\\epsilon) = 0$, entonces $E((\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon) = 0$ y, por lo tanto, $E(\\hat{\\beta}) = \\beta$. Esto implica que el estimador es insesgado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un poco de estadística\n",
    "\n",
    "$$\\hat{\\beta} = \\beta + (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon$$\n",
    "\n",
    "**Varianza**\n",
    "\n",
    "$$Var(\\hat{\\beta}) = Var(\\beta + (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon)$$\n",
    "$$Var(\\hat{\\beta}) = Var((\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon)$$\n",
    "\n",
    "**Propiedad:** Si $\\textbf{A}$ es una matriz constante y $\\textbf{b}$ un vector aleatorio, entonces $Var(\\textbf{A} \\textbf{b}) = \\textbf{A} Var(\\textbf{b}) \\textbf{A}^T$ (análogo a la versión escalar $Var(a b) = a^2 Var(b)$).\n",
    "\n",
    "Aplicado acá:\n",
    "\n",
    "$$Var(\\hat{\\beta}) = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T Var(\\epsilon) \\textbf{X} (\\textbf{X}^T \\textbf{X})^{-1}$$\n",
    "\n",
    "Como $Var(\\epsilon) = \\sigma^2 I$, entonces:\n",
    "\n",
    "$$Var(\\hat{\\beta}) = \\sigma^2 (\\textbf{X}^T \\textbf{X})^{-1}$$\n",
    "\n",
    "¿Cómo estimamos $\\sigma^2$? \n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{N - p - 1} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un poco de estadística\n",
    "\n",
    "#### Distribución\n",
    "\n",
    "Como los errores son normales, $\\epsilon \\sim N(0, \\sigma^2 I)$, entonces $$\\hat{\\beta} \\sim N(\\beta, (\\textbf{X}^T \\textbf{X})^{-1} \\sigma^2) $$\n",
    "\n",
    "**¿Por qué?**\n",
    "\n",
    "El estimador $\\hat{\\beta} = \\beta + (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\epsilon$ es una combinación lineal de variables aleatorias normales, por lo que también es normal. La esperanza y la varianza se obtienen directamente de la expresión anterior.\n",
    "\n",
    "**Propiedad:**\n",
    "Si $\\textbf{b}$ un vector aleatorio con distribución normal $N(\\mu, \\Sigma)$ y $\\textbf{A}$ una matriz constante, entonces $\\textbf{A} \\textbf{b} \\sim N(\\textbf{A} \\mu, \\textbf{A} \\Sigma \\textbf{A}^T)$.\n",
    "\n",
    "En este caso, $\\textbf{A} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un poco de estadística\n",
    "\n",
    "**Otros resultados relevantes:**\n",
    "1. El estimador de la variación de los errores $\\hat{\\sigma}^2$ sigue una distribución $\\chi^2$ con $N - p - 1$ grados de libertad.\n",
    "    $$ \\hat{\\sigma}^2 \\sim \\frac{\\sigma^2}{N - p - 1} \\chi^2_{N - p - 1}$$\n",
    "\n",
    "1. Para evaluar si un coeficiente $\\beta_j$ es significativo, se utiliza el *Z-score*:\n",
    "$$Z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{(\\textbf{X}^T \\textbf{X})^{-1}_{jj}}}$$\n",
    "Bajo la hipótesis nula, $Z_j$ sigue una distribución $t$ con $N - p - 1$ grados de libertad, que para grandes muestras se aproxima a una normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un poco de estadística\n",
    "\n",
    "**Otros resultados relevantes:**\n",
    "\n",
    "3. Estadístico $F$. Supongamos que queremos comparar un modelo con $p_1 + 1$ parámetros con otro modelo con $p_0 + 1$ parámetros ($p_0 < p_1$). Entonces, el estadístico $F$ se define como:\n",
    "$$F = \\frac{(RSS_0 - RSS_1) / (p_1 - p_0)}{RSS_1 / (N - p_1 - 1)}$$\n",
    "donde $RSS_0$ y $RSS_1$ son los residuos de los modelos con $p_0 + 1$ y $p_1 + 1$ parámetros, respectivamente. $F$ mide el cambio en la suma de residuos al cuadrado ($RSS_0 - RSS_1$) por cada parámetro adicional en el modelo ($p_1 - p_0$), normalizado por una estimación de la varianza ($RSS_1 / (N - p_1 - 1)$).\n",
    "\n",
    "Bajo la hipótesis nula de que el modelo más chico es correcto, $F$ sigue una distribución $F_{p_1 - p_0, N - p_1 - 1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Descenso por gradiente\n",
    "\n",
    "En conjuntos de datos muy grandes, el cálculo de la matriz $\\textbf{X}^T \\textbf{X}$ puede ser muy costoso. En estos casos, se puede utilizar un algoritmo de optimización como el descenso por gradiente para encontrar los coeficientes $\\beta$.\n",
    "\n",
    "**Descenso por gradiente estocástico**\n",
    "\n",
    "1. Inicializar $\\beta$ con valores aleatorios.\n",
    "1. En cada iteración $\\tau$:\n",
    "   1. Seleccionar una instancia aleatoria $i$ del conjunto de entrenamiento.\n",
    "   1. Calcular el error: $e_i = y_i - \\hat{y}_i = y_i - x_i^T \\beta$\n",
    "   1. Actualizar $\\beta^{(\\tau + 1)} = \\beta^{(\\tau)} + \\eta e_i x_i$\n",
    "\n",
    "\n",
    "¿Por qué es un *gradiente*? Considerar que si el *costo* está dado por la suma de los residuos al cuadrado, $RSS = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{N} (y_i - x_i^T \\beta)^2$, entonces la derivada respecto de $\\beta$ es $\\frac{\\partial RSS}{\\partial \\beta} = -2 \\sum_{i=1}^{N} (y_i - x_i^T \\beta) x_i$. Entonces, el gradiente es $-\\frac{1}{N} \\frac{\\partial RSS}{\\partial \\beta}$ y la actualización de $\\beta$ es $\\beta = \\beta - \\eta \\frac{1}{N} \\frac{\\partial RSS}{\\partial \\beta}$, donde $\\eta$ es la tasa de aprendizaje.\n",
    "\n",
    "Pero N puede ser muy grande, por lo que se puede usar un solo ejemplo para actualizar $\\beta$ en cada iteración. Entonces, el algoritmo se convierte en el descenso por gradiente estocástico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si sobrevivieron hasta acá, tratemos de bajar a tierra un poco todo esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicios\n",
    "\n",
    "**Ejercicio:** Dado un conjunto de valores $x_1, x_2, ..., x_N$\n",
    "1. ¿Qué valor $x$ minimiza el error cuadrático ($\\sum_{i=1}^{N} (x_i - x)^2$)?\n",
    "1. ¿Y si el error es absoluto ($\\sum_{i=1}^{N} |x_i - x|$)?\n",
    "\n",
    "**Ejercicio:** Considerar el caso para una única variable predictora $X_1$:\n",
    "1. Escribir la función de costo $RSS(\\beta)$ sin usar la notación matricial. ¿Cuántas componentes tiene $\\beta$?\n",
    "1. Expandir la expresión y mostrar que es una función cuadrática en $\\beta$.\n",
    "1. Encontrar el mínimo de la función cuadrática y mostrar que se obtiene la solución de mínimos cuadrados:\n",
    "\n",
    "$$ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}$$\n",
    "\n",
    "$$ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}$$\n",
    "\n",
    "1. ¿Cómo se modifican esas expresiones si consideramos predictores estandarizados?¿Y si la variable de respuesta también está estandarizada?\n",
    "4. Plantear la expresión para el caso de $p = 2$ variables predictoras. ¿Cuántas componentes tiene $\\beta$? ¿Cuántas ecuaciones hay que resolver para encontrar la solución de mínimos cuadrados? Reflexionar sobre la ventaja de usar la formulación matricial.\n",
    "\n",
    "**Ejercicio:** Utilizando el dataset de *Prostate Cancer*:\n",
    "1. Reproducir las Tabla 3.1 y 3.2 del libro Elements of Statistical Learning. Utilizar la librería `statsmodels` de Python para ajustar el modelo lineal. Comparar con los resultados obtenidos en el libro.\n",
    "1. Reproducir la Tabla 3.2 del libro Elements of Statistical Learning utilizando un estimador de cuadrados mínimos hecho exclusivamente con `numpy`. Comparar con los resultados obtenidos en el libro y con los obtenidos con `statsmodels`.\n",
    "1. Obtener el estadístico $F$ para el modelo completo y el modelo reducido sin las variables $age$, $lcp$, $gleason$ y $pgg45$. Comparar con el valor obtenido en el libro. Interpretar.\n",
    "1. Obtener el error en el conjunto de evaluación. Obtener el \"benchmark\" propuesto en el libro. Graficar las predicciones del modelo completo ($y$ vs $y_{pred}$) y del benchmark.\n",
    "\n",
    "**Ejercicio:** Utilizando el dataset de *Prostate Cancer*:\n",
    "1. Implementar un estimador de cuadrados mínimos con descenso por gradiente estocástico. Utilizar una tasa de aprendizaje $\\eta = 0.001$ y un número de iteraciones $iter = 10000$. Graficar la evolución de cada coeficiente $\\beta_j$ en función de las iteraciones y, en el mismo gráfico, comparar con los resultados obtenidos previamente o en el libro. **Algunas recomendaciones**: no olvidar escalar los predictores e iniciializar los coeficientes $\\beta$ a partir de una distribución normal con media cero y varianza uno.\n",
    "1. Probar diferentes tasas de aprendizaje y número de iteraciones. ¿Qué ocurre si la tasa de aprendizaje es muy baja? ¿Y si es muy alta? ¿Qué ocurre si el número de iteraciones es muy bajo? ¿Y si es muy alto?\n",
    "1. Probar diferentes inicializaciones de los coeficientes $\\beta_j$. ¿Qué ocurre si se inicializan todos en cero? ¿Y si se inicializan en valores aleatorios?\n",
    "1. Pensar e investigar cómo se podría mejorar este algoritmo. En particular, considerar condiciones de parada y técnicas por *batch*. No es necesario implementar ninguna de esas mejoras, pero sí pensar en cómo se podrían implementar y qué ventajas tendrían.\n",
    "\n",
    "**Ejercicio:** Ir a la documentación y al código de la [regresión lineal en Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Familiarizarse con ella. Leer el comentario acerca de la implementación en \"Notes\". Luego, ir al código y ver cómo está implementado el `.fit()`. ¿Qué diferencias hay con lo visto?\n",
    "\n",
    "**Ejercicio:** Ir a la documentación de la [regresión Ridge en Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge). Leer acerca de los *solvers* disponibles y sus diferencias. ¿Cómo se relacional con lo visto? Volveremos sobre este ejercicio.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
