{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaebc90c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Para la presentación, qué necesito?\n",
    "\n",
    "1. Explicar el tema:\n",
    "    a. En qué consiste? Lineas generales\n",
    "    b. Por qué nos sirve\n",
    "    c. Cómo se hace?\n",
    "3. Ejemplos en codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc1705",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Descomposición QR\n",
    "\n",
    "Por Marcelo Gabriel Ulrich\n",
    "\n",
    "Aprendizaje Automatico - Septiembre 2025\n",
    "\n",
    "UNSAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107cca6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Repaso\n",
    "\n",
    "Antes de arrancar, vamos con un repaso (rápido) de los temas necesarios para entender la descomposición QR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe744a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "Tenemos $X_{original}^T = (X_1, X_2,..., X_p) $ y el vector $Y = (y_1, y_2, ..., y_n)$. \n",
    "\n",
    "La regresión lineal asume $$ \\hat{Y}  = \\beta_0 + \\sum_{j=1}^{p} X_j \\beta_j$$\n",
    "$$ \\hat{Y} = X^T \\beta $$ $$\\text{ con }  X^T = [ 1 | X_{original}^T ]$$\n",
    "\n",
    "Por lo tanto,  buscamos estimar $\\beta$ que minimice una función de costos/error. En general usamos Minimos Cuadrados, que busca minimizar $$RSS(\\beta) = \\sum_{i=1}^{N} (y_i - \\hat{y}_i(\\beta))^2$$\n",
    "\n",
    "obteniendo así el Estimador de Cuadrados Minimos:\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ade55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## El tema es que...\n",
    "\n",
    "...en la practica, calcular la inversa de $X^TX$ presenta un problemón:\n",
    "\n",
    "Si X tiene multicolinealidad perfecta (es decir, hay un $X_i$ linealmente dependiente de las otras X), el determinante de la matriz $X^T X$ es cero. Eso rompe la formula de estimadores de beta (no se puede calcular la inversa), por lo que hay que arremangarse y hacer otras cosas (ejemplo, selección de variables con SVD/PCA, etc.).\n",
    "\n",
    "Pero que pasa en casos menos extremos, cuando estos vectores no son linealmente dependientes, pero son muy parecidos, es decir, con multicolinealidad imperfecta (alta correlación pero no 1)? El determinante se acerca mucho a cero, pero no es estrictamente cero.\n",
    "\n",
    "\n",
    "Otra forma de justificarlo es que la matriz $X^TX$ \"sufre\" de la cuadratura del numero de condición:\n",
    "* El numero de condición, $cond(A) = ||A|| ||A^{-1}||$, indica que tan sensible es la matriz a los errores/pequeñas perturbaciones en los datos de entrada.\n",
    "* Si el numero es cercano a 1, todo bien, mientras más grande, más sensible a las perturbaciones en los datos\n",
    "\n",
    "Esto genera un problema a la hora de calcular la inversa: un pequeño error de redondeo (por aritmetica de punto flotante) genera grandes cambios en el resultado. Es decir, el calculo de $(X^T X)^{-1}$ se vuelve numericamente inestable, lo que hace que la estimación de los betas se vuelva poco confiable.\n",
    "\n",
    "[EJEMPLO COMPUTACIONAL]\n",
    "\n",
    "La multicolinealidad es un problema muy frecuente en _la naturaleza_ (todo conjunto de datos no generado artificialmente), por lo que hace que la formula de estimación de los betas sea poco practica (no da soluciones confiables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443053bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Y ahora quien podrá ayudarnos?\n",
    "\n",
    "[insertar meme]\n",
    "\n",
    "Yo! La descomposición de matrices!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbed804",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Descomposición de matrices\n",
    "\n",
    "Al \"desarmar\" $X^T X$ en otras matrices de manera inteligente, podemos aprovechar ciertas propiedades de las matrices para aligerar los calculos y generar soluciones más estables, más rápidas, etc.\n",
    "\n",
    "Ya vimos SVD, en la que se descompone X en 3 matrices: dos ortogonales y una diagonal con valores singulare. La SVD es super completa y nos dá mucha info sobre la matriz que descompone. Sin embargo, en el contexto de la regresión lineal, es información redundante, por lo que podriamos aspirar a encontrar otras tecnicas de descomposición que prioricen otras cosas por sobre la información extra, como la estabilidad y/o tiempo de computo.\n",
    "\n",
    "Hoy les traigo otro metodo de descomposición de matrices muy usado, que promete ser más eficiente computacionalmente y más directo que SVD... La Descomposición QR.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ff300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Descomposición QR\n",
    "\n",
    "Así cómo en Singular Value Decomposition $X = U D V^T$, este metodo propone descomponer X en otras matrices. En particular\n",
    "$$ X = QR $$\n",
    "\n",
    "[MEME de OBVIO]\n",
    "\n",
    "La matriz $Q$ es de $N\\times P+1$ y ortogonal, es decir, sus columnas son linealmente independientes/ortogonales/\"perpendiculares\" y sus normas son 1.\n",
    "\n",
    "La matriz $R$ es de $P+1\\times P+1$, triangular superior y contiene coeficientes (ya vamos a ver qué son...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f8337",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descomposición QR: Ultilidad\n",
    "\n",
    "La gracia es que al reemplazar en la formula del Estimador de Cuadrados Minimos, genera lo siguiente:\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "$$\\hat{\\beta} = (\\textbf{(QR)}^T \\textbf{QR})^{-1} \\textbf{(QR)}^T \\textbf{y}$$\n",
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{Q}^T \\textbf{QR})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2058fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Descomposición QR: Ultilidad\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{Q}^T \\textbf{QR})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "Como $Q$ es ortogonal, vale que $Q^T Q = Q Q^T = I$, entonces:\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{I} \\textbf{R})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{R})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "$$\\hat{\\beta} = \\textbf{R}^{-1} (\\textbf{R}^T)^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "$$\\hat{\\beta} = \\textbf{R}^{-1} \\textbf{Q}^T \\textbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9b045",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descomposición QR: Ultilidad\n",
    "\n",
    "La gracia es que $R^{-1}$ es más estable numericamente que $(X^T X)^{-1}$. El calculo de la inversa de R es más \"resistente\" a errores de redondeo generados por la aritmetica de punto flotante del calculo computacional. Es decir, calcular $R^{-1}$ arrastra menos error por redondeo que calcular $(X^TX)^{-1}$, llegando a soluciones más confiables/estables.\n",
    "\n",
    "Pero... Por qué? Qué son Q y R?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579dc2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descomposición QR: Generar la matriz Q\n",
    "\n",
    "$Q$ es una matriz cuyas columnas $q_i$ son ortogonales entre sí, lo que vimos que es clave para simplificar las cuentas.\n",
    "\n",
    "Pero, cómo la generamos?\n",
    "\n",
    "Usando el algoritmo de Gram-Schmidt para ortogonalizar vectores!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0743e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algoritmo de Gram-Schmidt\n",
    "\n",
    "En el libro de Elements of Statistical Learning, se describe de la siguiente manera:\n",
    "\n",
    "Dado un conjunto de vectores columna de $X$, $\\{x_0, x_1, ..., x_p\\}$\n",
    "\n",
    "1. $z_0 = x_0 = 1$\n",
    "\n",
    "2. $\\forall j=1, 2, ..., p$:\n",
    "\n",
    "$$ \\gamma_{l,j} = \\frac{\\langle z_l,x_j \\rangle}{\\langle z_l,z_l \\rangle} \\space \\space \\forall l = 0, 1, ..., j-1$$\n",
    "\n",
    "$$ z_j = x_j - \\sum_{k=0}^{j-1} \\gamma_{k,j} z_k$$\n",
    "\n",
    "Entonces, la base de vectores residuos, $\\{z_0, z_1,..., z_p\\}$, es una base ortogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae825d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[meme] Oye, despacio cerebrito..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7afd1b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Qué son estos residuos? bueno, vamos paso a paso.\n",
    "\n",
    "1. Tenemos el vector $z_0 = x_0 = [1, 1, ..., 1]^T$\n",
    "\n",
    "2. Quiero generar un vector $z_1$ tome al vector $x_1$ y lo \"independice\" de $x_0$. Para eso, proyectamos $x_1$ en $x_0$ (para ver \"cuanto apunta\" $x_1$ en la dirección de $x_0$) y le resto a $x_1$ esa proyección. Así, me quedo con la componente de $x_1$ que es perpendicular a $x_0$, o sea, linealmente independiente.\n",
    "\n",
    "    2.1. Como sabemos por definición, la proyección del vector $x_1$ sobre $x_0$ es $\\hat{x}_{0, 1} = \\frac{\\langle x_0, x_1 \\rangle}{||x_0||^2} x_0 = \\frac{\\langle z_0, x_1 \\rangle}{||z_0||^2} z_0$\n",
    "\n",
    "    2.2. Entonces, obtenemos $z_1 = x_1 - \\hat{x}_{0,1}$\n",
    "\n",
    "[Animación de la proyección ortogonal]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bd291",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora, si quiero encontrar $z_2$, no solo voy a tener que \"independizarla\" de $x_0$/$z_0$, sino que tambien de $z_1$.\n",
    "\n",
    "Entonces $z_2 = x_2 - \\hat{x}_{0,2} - \\hat{x}_{1,2}$\n",
    "\n",
    "Con $ \\hat{x}_{0,2} = \\frac{\\langle z_0, x_2 \\rangle}{||z_0||^2} z_0$ y $ \\hat{x}_{1,2} = \\frac{\\langle z_1, x_2 \\rangle}{||z_1||^2} z_1$ \n",
    "\n",
    "[animación?]\n",
    "\n",
    "Generalizando para los $p$ vectores columna, obtenemos las expresiones vistas antes\n",
    "\n",
    "$$ \\gamma_{l,j} = \\frac{\\langle z_l,x_j \\rangle}{\\langle z_l,z_l \\rangle} = \\frac{\\langle z_l, x_j \\rangle}{||z_l||^2}\\space \\space \\forall l = 0, 1, ..., j-1$$\n",
    "\n",
    "$$ z_j = x_j - \\sum_{l=0}^{j-1} \\gamma_{l,j} z_l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f7a0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entonces, genero dos matrices:\n",
    "1. $Z$, con los residuos $z$ como columnas\n",
    "2. $\\Gamma$, una matriz triangular con los coeficientes $\\gamma$ y el resto 0 (recordar que calculamos $\\gamma_{0,1}$ y no $\\gamma_{1,0}$)\n",
    "\n",
    "Y obtengo $X = Z\\Gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c59ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validando \n",
    "\n",
    "Por definición: $ z_j = x_j - \\sum_{l=0}^{j-1} \\gamma_{l,j} z_l$, por lo que es lo mismo que \n",
    "\n",
    "$$ x_j = z_j + \\sum_{l=0}^{j-1} \\gamma_{l,j} z_l$$\n",
    "\n",
    "Para el caso $x_0$, $x_0 = z_0 = [z_0] [1]^T$\n",
    "\n",
    "Para el caso $x_1$, $x_1 = z_1 + \\gamma_{0,1} z_0 = [z_0 \\space z_1] [\\gamma_{0,1} \\space 1]^T$\n",
    "\n",
    "Para el caso $x_2$, $x_2 = z_2 + \\gamma_{0,2} z_0 + \\gamma_{0,1} z_1 = [z_0 \\space z_1 \\space z_2] [\\gamma_{0,2} \\space \\gamma_{1, 2} \\space 1]^T$\n",
    "\n",
    "En notación matricial vale \n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "z_0 & z_1 & z_2\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1 & \\gamma_{0,1} & \\gamma_{0,2} \\\\\n",
    "0 & 1 & \\gamma_{1,2} \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bed04b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahhh, o sea que $Q$ son los vectores columna $z$ y $R$ son los coeficientes $\\gamma_{l,j}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1764a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "No pero casi, sino se llamaría descomposición $Z\\Gamma$ xddd\n",
    "\n",
    "Estamos casi ahí, el unico problema es que Z no es una matriz ortogonal, ya que sus columnas no tienen norma 1 (nunca se lo pedimos a las z...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db141923",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para lograrlo, necesitamos dividir las componentes de $Z$ por sus normas.\n",
    "\n",
    "Cómo? Fácil, un poco de magia (o sea, algebra lineal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76e134",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Definimos una matriz diagonal D, en la cual $d_{i,i} = || z_i || $, por lo que $D^{-1}$ tiene los reciprocos de D, $d^*_{i,i} = \\frac{1}{||z_i||}$\n",
    "\n",
    "Si multiplicamos $Z$ y $D^{-1}$, obtenemos los vectores z_i normalizados, es decir:\n",
    "\n",
    "$$ q_i = \\frac{z_i}{||z_i||}$$\n",
    "\n",
    "Por lo que definimos\n",
    "\n",
    "$$ Q = [q_0 \\space q_1 \\space ... \\space q_p] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0449627",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cómo $D$ es diagonal, vale que $D^{-1} D = I$, por lo que nos lo podemos sacar del [REDACTED] la nada\n",
    "\n",
    "Entonces nos queda\n",
    "\n",
    "$$ X = Z I \\Gamma = (Z D^{-1}) (D \\Gamma)$$\n",
    "$$ X = Q R $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a4e59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entonces, que hay en $Q$ y $R$?\n",
    "\n",
    "$Q$ tiene columnas ortonormales generadas a partir de las columnas de $X$ a traves del algoritmo de Gram-Schmidt\n",
    "\n",
    "$R$ tiene en su diagonal principal las normas de los vectores $z_i$, es decir, de los residuos de ortogonalizar las columnas de X; y en el resto de su triangulo superior, los \"terminos cruzados\" de nuestras columnas $x_i$ originales, extraidos a partir de proyecciones ortogonales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944f7c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corre o no corre?\n",
    "\n",
    "[Armar codigo para probar casos de matrices rectangulares, ortogonales perfectas, casi ortogonales, multicolineales y casi multicolineales]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731113d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Muchas gracias por su atención!\n",
    "\n",
    "Preguntas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
