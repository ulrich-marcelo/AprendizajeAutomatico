{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaebc90c",
   "metadata": {},
   "source": [
    "Para la presentación, qué necesito?\n",
    "\n",
    "0. Repaso conceptual:\n",
    "    a. Matrices como Transformaciones lineales\n",
    "    b. Minimos cuadrados (notación matricial)\n",
    "1. Explicar el tema:\n",
    "    a. En qué consiste? Lineas generales\n",
    "    b. Por qué nos sirve\n",
    "    c. Cómo se hace?\n",
    "2. Autores del libro? No es paper, quizás es un extra si veo que no llego con el tiempo\n",
    "3. Ejemplos en codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107cca6",
   "metadata": {},
   "source": [
    "# Repaso\n",
    "\n",
    "Antes de arrancar, vamos con un repaso (rápido) de los temas necesarios para entender la descomposición QR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe744a",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "Tenemos $X_{original}^T = (X_1, X_2,..., X_p) $ y el vector $Y = (y_1, y_2, ..., y_n)$. \n",
    "\n",
    "La regresión lineal asume $$ \\hat{Y}  = \\beta_0 + \\sum_{j=1}^{p} X_j \\beta_j$$\n",
    "$$ \\hat{Y} = X^T \\beta $$ $$\\text{ con }  X^T = [ 1 | X_{original}^T ]$$\n",
    "\n",
    "Por lo tanto,  buscamos estimar $\\beta$ que minimice una función de costos/error. En general usamos Minimos Cuadrados, que busca minimizar $$RSS(\\beta) = \\sum_{i=1}^{N} (y_i - \\hat{y}_i(\\beta))^2$$\n",
    "\n",
    "obteniendo así el Estimador de Cuadrados Minimos:\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ade55",
   "metadata": {},
   "source": [
    "Sisi, todo muy bonito, y??\n",
    "\n",
    "El problema es que, en la practica, calcular la inversa de $X^TX$ presenta un problemón:\n",
    "\n",
    "Si X tiene multicolinealidad (es decir, hay un $X_i$ linealmente dependiente de las otras X), el determinante de la matriz $X^T X$ es cercano a cero.\n",
    "\n",
    "Eso hace que al calcular su inversa, un pequeño error de redondeo genere grandes cambios en el resultado de la inversa. Es decir, el calculo de $(X^T X)^{-1}$ se vuelve numericamente inestable, lo que hace que la estimación de los betas se vuelva poco confiable [si con un pequeño cambio en las X, nos da un numero muy diferente para los betas, tan confiable no era]\n",
    "\n",
    "[EJEMPLO COMPUTACIONAL]\n",
    "\n",
    "\n",
    "La multicolinealidad es un problema muy frecuente en _la naturaleza_ (todo conjunto de datos no generado artificialmente), por lo que hace que la formula de los betas sea poco practica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68084293",
   "metadata": {},
   "source": [
    "Y ahora quien podrá ayudarnos??\n",
    "\n",
    "[insertar meme]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbed804",
   "metadata": {},
   "source": [
    "Yo! La descomposición de matrices!\n",
    "\n",
    "Al \"desarmar\" $X^T X$ en otras matrices de manera inteligente, podemos aprovechar ciertas propiedades de las matrices para aligerar los calculos y generar soluciones más estables, más rápidas, etc.\n",
    "\n",
    "Ya vimos SVD, en la que ... [explicar brevemente SVD]. La SVD es super completa y nos dá mucha info sobre la matriz que descompone. Sin embargo, en el contexto de la regresión lineal, es información redundante. Informacion redundante = hacer cuentas innecesarias = buuuuuu\n",
    "\n",
    "hoy les traigo otro metodo de descomposición de matrices muy usado, que promete ser más estable y más rapido que SVD... La Descomposición QR.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ff300",
   "metadata": {},
   "source": [
    "# Descomposición QR\n",
    "\n",
    "Así cómo en Singular Value Deocmposition $X = U D V^T$, este metodo propone descomponer X en otras matrices. En particular\n",
    "$$ X = QR $$\n",
    "[MEME de OBVIO]\n",
    "\n",
    "La matriz Q es de NxP+1 y ortogonal (lease, vectores columna linealmente independientes).\n",
    "\n",
    "La matriz R es de P+1xP+1 y triangular superior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f8337",
   "metadata": {},
   "source": [
    "Ok, y???\n",
    "\n",
    "La gracia es que al reemplazar en la formula del Estimador de Cuadrados Minimos, genera lo siguiente:\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "$$\\hat{\\beta} = (\\textbf{(QR)}^T \\textbf{QR})^{-1} \\textbf{(QR)}^T \\textbf{y}$$\n",
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{Q}^T \\textbf{QR})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2058fc",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{Q}^T \\textbf{QR})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "Como $Q$ es ortogonal, vale que $Q^T Q = Q Q^T = I$, entonces:\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{I} \\textbf{R})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "$$\\hat{\\beta} = (\\textbf{R}^T \\textbf{R})^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "$$\\hat{\\beta} = \\textbf{R}^{-1} (\\textbf{R}^T)^{-1} \\textbf{R}^T \\textbf{Q}^T \\textbf{y}$$\n",
    "\n",
    "$$\\hat{\\beta} = \\textbf{R}^{-1} \\textbf{Q}^T \\textbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9b045",
   "metadata": {},
   "source": [
    "La gracia es que $R^{-1}$ es más estable numericamente que $(X^T X)^{-1}$\n",
    "\n",
    "Por qué? Qué son Q y R??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944f7c8",
   "metadata": {},
   "source": [
    "1. Explicar que son Q y R y cómo calcularlas (Algo Gram-Schmidt)\n",
    "2. Al mostrar qué tiene R, explicar porqué es más estable numericamente [INVESTIGAR https://gemini.google.com/app/a923a5e2a1b317f9?utm_source=app_launcher&utm_medium=owned&utm_campaign=base_all].\n",
    "3. Ver si librerias usan QR.\n",
    "\n",
    "$Q$ es una matriz cuyas columnas $q_i$ son ortogonales entre sí, lo que forma una base ortogonal del espacio vectorial generado por las columnas de $X$.\n",
    "\n",
    "Por qué es clave se preguntaran? Facilita los calculos para representar proyecciones en ese espacio.\n",
    "\n",
    "[Ejemplo]\n",
    "\n",
    "EXPLICAR:\n",
    "\n",
    "1. Recordar que lo que hacemos en MSE es proyectar vector y sobre el hiperplano generado por X, y encontramos las coordenadas $\\beta$ que minimizan la distancia entre $\\hat{y}$ e y; haciendo que $\\hat{y}$ sea efectivamente la proyección ortogonal.\n",
    "\n",
    "2. Que una base no sea ortogonal hace que, al calcular \"a donde van a parar\" los vectores en esa base (es decir, producto entre matriz de cambio de base y vector de interes), hace que haya \"interferencia\" entre los generadores del espacio vectorial [ejemplo]. Esto hace que haya que hacer más cuentas innecesarias (sumas y restas innecesarias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
